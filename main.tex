\documentclass[11pt]{article}

% ---- Packages ----
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsthm}
\usepackage{newtxtext}
\usepackage[amssymbols]{newtxmath} % Times font with full math support
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric}
\usepackage[colorlinks=true,linkcolor=blue!70!black,citecolor=blue!70!black,urlcolor=blue!70!black]{hyperref}
\usepackage{cite}
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}

% ---- Theorem environments ----
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

% ---- Notation shortcuts ----
\newcommand{\RD}{R(D)}
\newcommand{\Hp}{H(p)}
\newcommand{\HD}{H(D)}
\newcommand{\Ber}{\mathrm{Bernoulli}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calXhat}{\hat{\mathcal{X}}}
\newcommand{\Qinv}{Q^{-1}}
\DeclareMathOperator*{\argmin}{arg\,min}

% ---- Title ----
\title{\LARGE Finite Block Length Rate-Distortion Theory\\for the Bernoulli Source:\\A Tutorial}
\author{Bhaskar Krishnamachari}
\date{February 26, 2026}

\begin{document}
\maketitle

% ========================================================================
% ABSTRACT
% ========================================================================
\begin{abstract}
Lossy data compression lies at the heart of modern communication and storage systems.
Shannon's rate-distortion theory provides the fundamental limit on how much a source
can be compressed at a given fidelity, but it assumes infinitely long block lengths
that are never realized in practice.
We present a self-contained tutorial on rate-distortion theory for the simplest
non-trivial source: a Bernoulli$(p)$ sequence with Hamming distortion.
We derive the classical rate-distortion function $\RD = \Hp - \HD$ from first
principles, illustrate its computation via the Blahut-Arimoto algorithm, and then
develop the finite block length refinements that characterize how the minimum
achievable rate approaches the Shannon limit as the block length $n$ grows.
The central quantity in this refinement is the \emph{rate-distortion dispersion}
$V(D)$, which governs the $O(1/\sqrt{n})$ penalty for operating at finite block
lengths.
We accompany all theoretical developments with numerical examples and figures
generated by accompanying Python scripts.
\end{abstract}

% ========================================================================
% SECTION 1: INTRODUCTION
% ========================================================================
\section{Introduction}
\label{sec:introduction}

The theory of lossy data compression traces its origins to Claude Shannon's
landmark 1948 paper~\cite{shannon1948}, which established that every source has
a well-defined minimum description rate for any prescribed level of
distortion.
This result, made precise in Shannon's 1959 coding theorem for sources with a
fidelity criterion~\cite{shannon1959}, was remarkable for a reason that is easy
to overlook today: it demonstrated that a single, clean mathematical function,
the rate-distortion function $\RD$, separates the achievable from the impossible, no
matter how clever the compression scheme.

However, this elegant theory rests on a crucial idealization.
The rate-distortion function $\RD$ is an \emph{asymptotic} quantity: it describes
the minimum rate achievable when the block length $n$ tends to infinity.
Real communication and storage systems must operate with finite memory, finite
latency, and finite computational resources.
A natural and practically important question therefore arises: \emph{how much
extra rate do we need when the block length is finite?}

Over the past two decades, a precise answer to this question has emerged through
the work of Strassen~\cite{strassen1962}, Kontoyiannis and Verd\'{u}~\cite{kontoyiannis2014},
Kostina and Verd\'{u}~\cite{kostina2012}, and others.
To state their result, we need two preliminary ideas.

The first is a \emph{distortion measure}: a function $d(x, \hat{x})$ that
quantifies how ``far'' a reconstruction symbol $\hat{x}$ is from the original
source symbol $x$.
For binary data, the simplest and most natural choice is the \emph{Hamming
distortion}, which equals $1$ when $x \neq \hat{x}$ and $0$ when $x = \hat{x}$.
When we compress a length-$n$ source sequence $X^n = (X_1, \ldots, X_n)$ into
a reconstruction $\hat{X}^n = (\hat{X}_1, \ldots, \hat{X}_n)$, the overall
quality is measured by the \emph{per-symbol distortion}
$\frac{1}{n}\sum_{i=1}^{n} d(X_i, \hat{X}_i)$,
which under Hamming distortion is simply the fraction of positions where the
source and reconstruction disagree (the bit error rate).

The second idea is that we must refine what ``achievable'' means at finite
block length.
When $n$ is finite, no code can guarantee that \emph{every} source sequence is
reproduced within distortion $D$.
The source sequence $X^n$ is random, and some realizations are inherently harder
to compress than others.
We therefore allow a small probability of failure: we require only that the
distortion exceeds $D$ with probability at most $\varepsilon$.
More precisely, we say a code is $(n, D, \varepsilon)$-achievable at rate $R$
if
\begin{equation}
\label{eq:excess_distortion_intro}
\Prob\!\left(\frac{1}{n}\sum_{i=1}^{n} d(X_i, \hat{X}_i) > D\right) \leq \varepsilon,
\end{equation}
where the probability is over the randomness of the source sequence $X^n$.
The quantity $\varepsilon$ is called the \emph{excess-distortion probability}.
With this formulation, the minimum achievable rate $R(n, D, \varepsilon)$
depends on three parameters: the block length $n$, the target distortion $D$,
and the tolerated failure probability $\varepsilon$.

The key insight of the finite block length theory is that $R(n, D, \varepsilon)$
admits a clean asymptotic expansion:
\begin{equation}
\label{eq:normal_approx_intro}
R(n, D, \varepsilon) \approx \RD + \sqrt{\frac{V(D)}{n}}\, \Qinv(\varepsilon),
\end{equation}
where $V(D)$ is the \emph{rate-distortion dispersion}, a quantity that captures
how variable the compression difficulty is across source symbols, and
$\Qinv(\varepsilon)$ is the inverse of the Gaussian $Q$-function.
From an engineering standpoint, (\ref{eq:normal_approx_intro}) reveals that the
penalty for finite block length decays as $1/\sqrt{n}$, a rate that is neither
negligibly fast nor prohibitively slow.

In this tutorial, we develop the entire story from first principles for the simplest
non-trivial setting: a $\Ber(p)$ source with Hamming distortion.
We have chosen this source for three reasons.
First, the binary symmetric source is the discrete analogue of the Gaussian source:
it is the canonical ``textbook'' example against which all intuitions are calibrated.
Second, every quantity of interest ($\RD$, the optimal test channel, the $d$-tilted
information, and the dispersion $V(D)$) admits a clean closed-form expression.
Third, despite its simplicity, the Bernoulli source reveals the full structure of
the finite block length theory, including the role of source dispersion as a
second-order characterization of compression difficulty.

The key contributions of this tutorial are:
\begin{enumerate}
    \item A self-contained derivation of the rate-distortion function $\RD = \Hp - \HD$
          for the $\Ber(p)$ source, accessible to readers with minimal probability background.
    \item A detailed treatment of the Blahut-Arimoto algorithm, including explicit
          $2 \times 2$ matrix computations and convergence analysis.
    \item A development of finite block length rate-distortion theory, including the
          $d$-tilted information, rate-distortion dispersion, and the normal approximation.
    \item Accompanying Python scripts that reproduce all numerical results and figures.
\end{enumerate}

The remainder of this paper is organized as follows.
Section~\ref{sec:foundations} reviews the probability and information-theoretic
foundations.
Section~\ref{sec:rd_problem} formulates the rate-distortion problem.
Section~\ref{sec:bernoulli_rd} derives the rate-distortion function for the
Bernoulli source.
Section~\ref{sec:blahut_arimoto} presents the Blahut-Arimoto algorithm.
Section~\ref{sec:finite_blocklength} develops the finite block length theory.
Section~\ref{sec:numerical} presents comprehensive numerical explorations.
Finally, Section~\ref{sec:conclusion} concludes with a discussion of open
problems and further reading.


% ========================================================================
% SECTION 2: PROBABILITY AND INFORMATION FOUNDATIONS
% ========================================================================
\section{Probability and Information Foundations}
\label{sec:foundations}

In this section, we review the essential probability and information-theoretic
concepts that underpin rate-distortion theory.
We aim for an intuitive, example-driven development; readers seeking formal
generality may consult Cover and Thomas~\cite{cover2006}.

% ---- 2.1 ----
\subsection{Random Variables and Probability}
\label{subsec:rv}

Consider a coin that lands heads with probability $p$ and tails with
probability $1 - p$, where $0 \leq p \leq 1$.
If we encode heads as $1$ and tails as $0$, a single coin flip is described
by a \emph{Bernoulli random variable} $X$ with
\begin{equation}
\Prob(X = 1) = p, \qquad \Prob(X = 0) = 1 - p.
\end{equation}
We write $X \sim \Ber(p)$ and refer to $p$ as the \emph{bias} of the source.
A fair coin corresponds to $p = 1/2$, while a biased coin has $p \neq 1/2$.

When we flip the coin $n$ times independently, we obtain a \emph{sequence}
$X^n = (X_1, X_2, \ldots, X_n)$ of independent and identically distributed
(i.i.d.) Bernoulli random variables.
This sequence is the object we wish to compress.

% ---- 2.2 ----
\subsection{Entropy}
\label{subsec:entropy}

How much ``surprise'' does a single coin flip carry?
If the coin always lands heads ($p = 1$), there is no surprise at all; we know
the outcome in advance.
If the coin is fair ($p = 1/2$), each flip is maximally uncertain.
Shannon formalized this intuition through the \emph{entropy} of a random variable.

\begin{definition}[Binary Entropy]
\label{def:binary_entropy}
The \emph{binary entropy function} is defined as
\begin{equation}
\label{eq:binary_entropy}
H(p) = -p \log_2 p - (1-p) \log_2 (1-p),
\end{equation}
with the convention that $0 \log_2 0 = 0$.
\end{definition}

The entropy $\Hp$ measures the average surprise, in bits, of a single draw
from a $\Ber(p)$ source.
It is zero when $p = 0$ or $p = 1$ (no uncertainty) and achieves its maximum
value of $1$ bit when $p = 1/2$ (maximum uncertainty).
Figure~\ref{fig:binary_entropy} illustrates this behavior.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/binary_entropy.pdf}
    \caption{The binary entropy function $\Hp$ versus the source bias $p$.
    The entropy is maximized at $p = 1/2$, where each bit carries one full bit
    of information, and vanishes at $p \in \{0, 1\}$, where the source is
    deterministic.}
    \label{fig:binary_entropy}
\end{figure}

More generally, for a discrete random variable $X$ taking values in a finite
alphabet $\calX$ with probability mass function $p_X(x)$, the entropy is
\begin{equation}
\label{eq:entropy_general}
H(X) = -\sum_{x \in \calX} p_X(x) \log_2 p_X(x).
\end{equation}
The entropy quantifies the minimum average number of bits needed to losslessly
represent $X$.

% ---- 2.3 ----
\subsection{Sequences and Typical Sequences}
\label{subsec:typical}

Consider a sequence $x^n = (x_1, \ldots, x_n)$ produced by a $\Ber(p)$ source.
The \emph{type} of $x^n$ is the empirical distribution of symbols: if $x^n$
contains $k$ ones, its type is $k/n$.
For large $n$, the law of large numbers guarantees that the fraction of ones
concentrates around $p$.

A sequence is called \emph{typical} if its empirical statistics are close to
the true source distribution.
The \emph{asymptotic equipartition property} (AEP) states that with high
probability, a sequence drawn from a $\Ber(p)$ source satisfies
\begin{equation}
-\frac{1}{n} \log_2 p_{X^n}(X^n) \approx \Hp.
\end{equation}
Intuitively, there are approximately $2^{n\Hp}$ typical sequences, and they
account for almost all of the probability mass.
This observation is the foundation of both lossless and lossy compression.

% ---- 2.4 ----
\subsection{Mutual Information}
\label{subsec:mutual_info}

When we compress a source $X$ into a reconstruction $\hat{X}$, some information
about $X$ is preserved and some is lost.
The \emph{mutual information} $I(X; \hat{X})$ quantifies how much information
$\hat{X}$ retains about $X$.

\begin{definition}[Mutual Information]
\label{def:mutual_info}
For jointly distributed random variables $(X, \hat{X})$ with joint distribution
$p_{X,\hat{X}}(x, \hat{x})$, the mutual information is
\begin{equation}
\label{eq:mutual_info}
I(X; \hat{X}) = \sum_{x, \hat{x}} p_{X,\hat{X}}(x, \hat{x})
\log_2 \frac{p_{\hat{X}|X}(\hat{x}|x)}{p_{\hat{X}}(\hat{x})}.
\end{equation}
\end{definition}

An equivalent and often more intuitive expression is
\begin{equation}
\label{eq:mi_entropy}
I(X; \hat{X}) = H(X) - H(X | \hat{X}),
\end{equation}
where $H(X|\hat{X})$ is the conditional entropy, that is, the residual uncertainty
about $X$ after observing $\hat{X}$.
The mutual information is always non-negative, and equals zero if and only if
$X$ and $\hat{X}$ are independent.
It equals $H(X)$ when $\hat{X}$ determines $X$ perfectly.


% ========================================================================
% SECTION 3: THE RATE-DISTORTION PROBLEM
% ========================================================================
\section{The Rate-Distortion Problem}
\label{sec:rd_problem}

In this section, we formulate the central problem of lossy compression.
We define what it means to compress a source with a prescribed fidelity, and
state Shannon's fundamental theorem that establishes the existence of a minimum
achievable rate.

% ---- 3.1 ----
\subsection{What Is Lossy Compression?}
\label{subsec:lossy}

Consider a source that emits a sequence $X^n = (X_1, \ldots, X_n)$ of $n$
i.i.d.\ $\Ber(p)$ random variables.
A \emph{lossy compression scheme} consists of two mappings:
\begin{itemize}
    \item An \emph{encoder} $f_n : \{0,1\}^n \to \{1, 2, \ldots, M\}$ that maps
          the source sequence to one of $M$ codewords.
    \item A \emph{decoder} $g_n : \{1, 2, \ldots, M\} \to \{0,1\}^n$ that maps
          each codeword index back to a reconstruction sequence
          $\hat{X}^n = g_n(f_n(X^n))$.
\end{itemize}
The set $\mathcal{C} = \{g_n(1), g_n(2), \ldots, g_n(M)\}$ is called the
\emph{codebook}, and each element is a \emph{reproduction sequence}.

The \emph{rate} of the code is
\begin{equation}
\label{eq:rate_def}
R = \frac{1}{n} \log_2 M \quad \text{bits per source symbol}.
\end{equation}
This quantity measures the average number of bits used to represent each source
symbol.
A lower rate means more aggressive compression.

% ---- 3.2 ----
\subsection{Distortion Measures}
\label{subsec:distortion}

To quantify the fidelity of the reconstruction, we need a \emph{distortion measure}.
For binary sequences, the natural choice is the \emph{Hamming distortion}:
\begin{equation}
\label{eq:hamming}
d(x, \hat{x}) =
\begin{cases}
0, & \text{if } x = \hat{x}, \\
1, & \text{if } x \neq \hat{x}.
\end{cases}
\end{equation}
The Hamming distortion simply counts whether a symbol was reproduced correctly.

For a pair of sequences $(x^n, \hat{x}^n)$, the \emph{per-symbol distortion} is
\begin{equation}
\label{eq:block_distortion}
d(x^n, \hat{x}^n) = \frac{1}{n} \sum_{i=1}^{n} d(x_i, \hat{x}_i),
\end{equation}
which equals the fraction of positions where the source and reconstruction
disagree, that is, the \emph{bit error rate}.

% ---- 3.3 ----
\subsection{The Fundamental Question}
\label{subsec:fundamental}

We can now state the central question of rate-distortion theory:

\medskip
\noindent\emph{What is the minimum rate $R$ such that there exists a sequence of
encoder-decoder pairs achieving average distortion at most $D$?}
\medskip

Shannon's remarkable insight was that this minimum rate is given by a single-letter
optimization, independent of the block length~\cite{shannon1959}.
We do not need to search over all possible encoder-decoder pairs of all possible
block lengths.
Instead, the answer depends only on the source distribution $p_X$ and the
distortion measure $d$.

% ---- 3.4 ----
\subsection{The Test Channel and the Rate-Distortion Function}
\label{subsec:test_channel}

The key to Shannon's formulation is the concept of a \emph{test channel}.
Rather than optimizing over encoder-decoder pairs, we optimize over conditional
distributions $p_{\hat{X}|X}(\hat{x}|x)$ that describe a probabilistic mapping
from source symbols to reconstruction symbols.

\begin{definition}[Rate-Distortion Function]
\label{def:rd_function}
The rate-distortion function of a source $X$ with distortion measure $d$ is
\begin{equation}
\label{eq:rd_function}
\RD = \min_{\substack{p_{\hat{X}|X}: \\ \E[d(X, \hat{X})] \leq D}} I(X; \hat{X}),
\end{equation}
where the minimization is over all conditional distributions $p_{\hat{X}|X}$
satisfying the distortion constraint.
\end{definition}

The rate-distortion function has a clean operational interpretation: $\RD$ is
the minimum number of bits per source symbol required to describe the source
with average distortion at most $D$.
Rates above $\RD$ are achievable (there exist codes that work), while rates
below $\RD$ are not achievable by any code, regardless of its complexity.

Figure~\ref{fig:test_channel} illustrates the encoder-decoder structure and the
test channel abstraction.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[>=Stealth, node distance=1.4cm, font=\small]
        % Source
        \node[draw, rounded corners, fill=blue!8, minimum width=1.3cm,
              minimum height=0.8cm] (src) {Source};
        % Encoder
        \node[draw, rounded corners, fill=orange!12, minimum width=1.3cm,
              minimum height=0.8cm, right=of src] (enc) {Encoder};
        % Channel
        \node[right=1.1cm of enc] (mid) {};
        % Decoder
        \node[draw, rounded corners, fill=green!10, minimum width=1.3cm,
              minimum height=0.8cm, right=2.2cm of enc] (dec) {Decoder};
        % Output
        \node[right=0.8cm of dec] (out) {$\hat{X}^n$};

        % Arrows
        \draw[->] (src) -- node[above]{\footnotesize $X^n$} (enc);
        \draw[->] (enc) -- node[above]{\footnotesize $nR$ bits} (dec);
        \draw[->] (dec) -- (out);

        % Test channel below
        \node[below=2.0cm of src, xshift=1.0cm] (x) {$X$};
        \node[draw, rounded corners, fill=purple!10, minimum width=2.2cm,
              minimum height=0.8cm, right=0.8cm of x] (tc) {$p_{\hat{X}|X}$};
        \node[right=0.8cm of tc] (xhat) {$\hat{X}$};
        \draw[->] (x) -- (tc);
        \draw[->] (tc) -- (xhat);

        % Labels
        \node[above=0.15cm of enc, xshift=0.3cm, gray] {\footnotesize Operational};
        \node[above=0.15cm of tc, gray] {\footnotesize Test Channel};
    \end{tikzpicture}
    \caption{Top: the operational lossy compression setup with encoder and decoder.
    Bottom: the test channel $p_{\hat{X}|X}$ that abstracts away the codebook
    structure.  The rate-distortion function minimizes mutual information
    $I(X;\hat{X})$ over all test channels satisfying the distortion constraint.}
    \label{fig:test_channel}
\end{figure}


% ========================================================================
% SECTION 4: RATE-DISTORTION FOR THE BERNOULLI SOURCE
% ========================================================================
\section{The Rate-Distortion Function for the Bernoulli Source}
\label{sec:bernoulli_rd}

In this section, we derive the rate-distortion function for the $\Ber(p)$
source with Hamming distortion.
This is perhaps the cleanest closed-form result in all of rate-distortion theory.

% ---- 4.1 ----
\subsection{Setting Up the Optimization}
\label{subsec:rd_setup}

We wish to solve
\begin{equation}
\label{eq:rd_bernoulli_opt}
\RD = \min_{\substack{p_{\hat{X}|X}: \\ \E[d(X, \hat{X})] \leq D}} I(X; \hat{X})
\end{equation}
for $X \sim \Ber(p)$ and Hamming distortion $d(x, \hat{x}) = \mathbf{1}\{x \neq \hat{x}\}$.

Since both the source alphabet $\calX = \{0, 1\}$ and the reproduction alphabet
$\calXhat = \{0, 1\}$ are binary, the test channel $p_{\hat{X}|X}$ is a $2 \times 2$
stochastic matrix with four parameters, of which two are free (each row sums to one).
We can parameterize the test channel as
\begin{equation}
\label{eq:test_channel_matrix}
\begin{pmatrix}
p_{\hat{X}|X}(0|0) & p_{\hat{X}|X}(1|0) \\
p_{\hat{X}|X}(0|1) & p_{\hat{X}|X}(1|1)
\end{pmatrix}
=
\begin{pmatrix}
1 - \alpha & \alpha \\
\beta & 1 - \beta
\end{pmatrix},
\end{equation}
where $\alpha = p_{\hat{X}|X}(1|0)$ is the probability of flipping a $0$ to a $1$,
and $\beta = p_{\hat{X}|X}(0|1)$ is the probability of flipping a $1$ to a $0$.

The expected distortion under this test channel is
\begin{equation}
\E[d(X, \hat{X})] = (1-p)\alpha + p\beta.
\end{equation}
We seek the test channel parameters $(\alpha, \beta)$ that minimize the mutual
information $I(X; \hat{X})$ subject to $(1-p)\alpha + p\beta \leq D$.

% ---- 4.2 ----
\subsection{The Optimal Test Channel}
\label{subsec:optimal_test_channel}

To solve the optimization~(\ref{eq:rd_bernoulli_opt}), we form the Lagrangian
\begin{equation}
\label{eq:lagrangian}
\mathcal{L} = I(X; \hat{X}) + \lambda \bigl(\E[d(X, \hat{X})] - D\bigr),
\end{equation}
where $\lambda \geq 0$ is the Lagrange multiplier.
The Karush-Kuhn-Tucker (KKT) conditions for optimality yield a structural
result that is central to rate-distortion theory.

It turns out that the optimal test channel is a \emph{binary symmetric channel}
(BSC) with crossover probability equal to the target distortion $D$.
That is, the optimal solution has $\alpha = \beta = D$, giving
\begin{equation}
\label{eq:optimal_test_channel}
p^*_{\hat{X}|X} =
\begin{pmatrix}
1 - D & D \\
D & 1 - D
\end{pmatrix}.
\end{equation}
To build intuition for why this is the case, observe that the BSC treats both
source symbols symmetrically: regardless of whether the input is $0$ or $1$,
the channel flips it with probability $D$.
This symmetry is natural because the Hamming distortion itself treats errors
symmetrically.
Any asymmetric test channel (with $\alpha \neq \beta$) would ``waste'' distortion
budget by allocating more error probability to one symbol than the other, without
any compensating reduction in mutual information.

The corresponding Lagrange multiplier is
\begin{equation}
\label{eq:lambda_star}
\lambda^* = \ln \frac{1-D}{D},
\end{equation}
where $\ln$ denotes the natural logarithm.
Note that $\lambda^*$ is the negative of the slope of the $\RD$ curve.

% ---- 4.3 ----
\subsection{The Closed-Form Result}
\label{subsec:rd_closedform}

Having identified the optimal test channel as BSC($D$), we can now compute the
rate-distortion function in closed form.
When $X \sim \Ber(p)$ is passed through a BSC($D$), the output
$\hat{X} \sim \Ber(p * D)$, where $p * D = p(1-D) + (1-p)D$ denotes the
\emph{binary convolution}.
The mutual information evaluates to
\begin{equation}
I(X; \hat{X}) = H(\hat{X}) - H(\hat{X}|X) = H(p * D) - H(D).
\end{equation}
However, since we are minimizing over $D$ at the constraint boundary, and
$H(p * D) = \Hp$ is not quite right; let us be precise.

The mutual information through a BSC($D$) with $\Ber(p)$ input is
\begin{equation}
I(X; \hat{X}) = H(p * D) - H(D).
\end{equation}
When $D \leq \min(p, 1-p)$, the distortion constraint is active and the
rate-distortion function is
\begin{equation}
\label{eq:rd_bernoulli}
\boxed{\RD = \Hp - \HD, \qquad 0 \leq D \leq \min(p, 1-p).}
\end{equation}
For $D > \min(p, 1-p)$, we have $\RD = 0$.

\begin{remark}
The formula $\RD = \Hp - \HD$ has an appealing interpretation: the rate equals
the entropy of the source minus the entropy of the ``noise'' introduced by the
test channel.
As the allowed distortion $D$ increases, the noise entropy $\HD$ grows, and
fewer bits are needed to describe the source.
\end{remark}

Let us examine the boundary cases.
When $D = 0$, we require perfect reconstruction, and $R(0) = \Hp$: the rate
equals the source entropy, which is the lossless compression limit.
When $D = \min(p, 1-p)$, the rate drops to zero.
In this regime, the decoder can simply output the more likely symbol ($0$ if
$p < 1/2$, or $1$ if $p > 1/2$) for every position, achieving distortion
$\min(p, 1-p)$ with zero rate.

Figure~\ref{fig:rd_curves} shows the rate-distortion function for several values
of the source bias $p$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/rate_distortion_curves.pdf}
    \caption{The rate-distortion function $\RD = \Hp - \HD$ for a $\Ber(p)$
    source with Hamming distortion, shown for $p \in \{0.11, 0.2, 0.3, 0.5\}$.
    Each curve is convex and decreasing, starting at $R(0) = \Hp$ and reaching
    zero at $D = \min(p, 1-p)$.
    The $p = 0.5$ curve starts highest because the fair coin has the most entropy.}
    \label{fig:rd_curves}
\end{figure}

We note three important properties of the rate-distortion function:
\begin{enumerate}
    \item \textbf{Convexity:} $\RD$ is a convex function of $D$.
          This means that each additional unit of distortion ``buys'' progressively
          less rate reduction.
    \item \textbf{Monotonicity:} $\RD$ is non-increasing in $D$.
          Allowing more distortion can only help (or leave unchanged) the compression rate.
    \item \textbf{Continuity:} $\RD$ is continuous on $[0, \min(p, 1-p)]$.
\end{enumerate}

% ---- 4.4 ----
\subsection{Historical Note}
\label{subsec:rd_history}

Shannon stated the rate-distortion function for the binary source in his 1959
paper~\cite{shannon1959}.
A comprehensive treatment of rate-distortion theory for general sources was
developed by Berger~\cite{berger1971}.
The elegant formula $\RD = \Hp - \HD$ serves as the starting point for
virtually every textbook discussion of lossy source coding; see, for
example, Cover and Thomas~\cite{cover2006}.


% ========================================================================
% SECTION 5: THE BLAHUT-ARIMOTO ALGORITHM
% ========================================================================
\section{The Blahut-Arimoto Algorithm}
\label{sec:blahut_arimoto}

In this section, we present the Blahut-Arimoto algorithm, a powerful iterative
method for computing rate-distortion functions.
While the Bernoulli source admits a closed-form solution, the Blahut-Arimoto
algorithm applies to arbitrary finite-alphabet sources and distortion measures.

% ---- 5.1 ----
\subsection{Motivation}
\label{subsec:ba_motivation}

The rate-distortion function~(\ref{eq:rd_function}) is defined as a minimization
of mutual information over a convex set of conditional distributions.
For the Bernoulli source with Hamming distortion, we exploited the problem's
symmetry to obtain a closed-form solution.
However, for more complex sources, such as non-uniform discrete sources with
non-binary alphabets or non-Hamming distortion measures, the optimization
does not admit a closed-form solution, and a computational approach is needed.

The Blahut-Arimoto algorithm~\cite{blahut1972,arimoto1972} solves this
optimization through an elegant \emph{alternating minimization} procedure.
It was independently discovered by Blahut and Arimoto in 1972, and its
convergence was later established rigorously by Csisz\'{a}r~\cite{csiszar1974}.

% ---- 5.2 ----
\subsection{The Algorithm}
\label{subsec:ba_algorithm}

The Blahut-Arimoto algorithm operates on the Lagrangian dual formulation of
the rate-distortion problem.
For a given slope parameter $s > 0$ (corresponding to the Lagrange multiplier),
the algorithm minimizes the functional
\begin{equation}
\label{eq:ba_functional}
F(s) = \min_{p_{\hat{X}|X}} \bigl[ I(X; \hat{X}) + s \cdot \E[d(X, \hat{X})] \bigr].
\end{equation}
By sweeping $s$ over positive values, we trace out the entire $\RD$ curve.

The algorithm alternates between two updates:

\smallskip
\noindent\textbf{Step 1: Update the test channel.}
Given the current reproduction distribution $p_{\hat{X}}(\hat{x})$, update
\begin{equation}
\label{eq:ba_step1}
p_{\hat{X}|X}(\hat{x}|x) = \frac{p_{\hat{X}}(\hat{x})\, e^{-s\, d(x,\hat{x})}}{Z(x)},
\end{equation}
where $Z(x) = \sum_{\hat{x}} p_{\hat{X}}(\hat{x})\, e^{-s\, d(x,\hat{x})}$ is
a normalization constant ensuring $\sum_{\hat{x}} p_{\hat{X}|X}(\hat{x}|x) = 1$.

\smallskip
\noindent\textbf{Step 2: Update the reproduction distribution.}
Given the updated test channel, compute
\begin{equation}
\label{eq:ba_step2}
p_{\hat{X}}(\hat{x}) = \sum_{x} p_X(x)\, p_{\hat{X}|X}(\hat{x}|x).
\end{equation}
This is simply the marginal of $\hat{X}$ induced by passing $X$ through the
updated test channel.

The algorithm is initialized with a uniform reproduction distribution
$p_{\hat{X}}(\hat{x}) = 1/|\calXhat|$ and iterates Steps~1 and~2 until
convergence.

\begin{remark}
The alternating structure of the algorithm has a natural interpretation:
Step~1 finds the best test channel for a fixed output distribution, while
Step~2 updates the output distribution to be consistent with the new test channel.
This is an instance of the classical alternating minimization framework, and
convergence is guaranteed because each step decreases the Lagrangian objective.
\end{remark}

Algorithm~\ref{alg:ba} presents the complete pseudocode.

\begin{algorithm}[t]
\caption{Blahut-Arimoto Algorithm}
\label{alg:ba}
\begin{algorithmic}[1]
\Require Source distribution $p_X$, distortion matrix $d$, slope $s > 0$, tolerance $\delta$
\Ensure Rate $R$ and distortion $D$ on the $\RD$ curve
\State Initialize $p_{\hat{X}}(\hat{x}) \gets 1/|\calXhat|$ for all $\hat{x}$
\Repeat
    \For{each $x \in \calX$}
        \State $Z(x) \gets \sum_{\hat{x}} p_{\hat{X}}(\hat{x})\, e^{-s\, d(x,\hat{x})}$
        \For{each $\hat{x} \in \calXhat$}
            \State $p_{\hat{X}|X}(\hat{x}|x) \gets p_{\hat{X}}(\hat{x})\, e^{-s\, d(x,\hat{x})} / Z(x)$
        \EndFor
    \EndFor
    \For{each $\hat{x} \in \calXhat$}
        \State $p_{\hat{X}}(\hat{x}) \gets \sum_{x} p_X(x)\, p_{\hat{X}|X}(\hat{x}|x)$
    \EndFor
    \State Compute $R \gets I(X; \hat{X})$ and $D \gets \E[d(X,\hat{X})]$
\Until{$|R_{\text{new}} - R_{\text{old}}| < \delta$}
\State \Return $(R, D)$
\end{algorithmic}
\end{algorithm}

% ---- 5.3 ----
\subsection{Application to the Bernoulli Source}
\label{subsec:ba_bernoulli}

We now apply the Blahut-Arimoto algorithm to the $\Ber(p)$ source with Hamming
distortion.
Since the source and reproduction alphabets are both $\{0, 1\}$, the test
channel is a $2 \times 2$ matrix, and all computations are explicit.

Consider a concrete example with $p = 0.3$ and slope parameter $s = 10$.
At each iteration, we maintain the reproduction distribution
$(p_{\hat{X}}(0), p_{\hat{X}}(1))$ and the test channel matrix.
The updates in~(\ref{eq:ba_step1}) and~(\ref{eq:ba_step2}) become:
\begin{align}
p_{\hat{X}|X}(0|0) &= \frac{p_{\hat{X}}(0)}{p_{\hat{X}}(0) + p_{\hat{X}}(1)\, e^{-s}}, \\
p_{\hat{X}|X}(0|1) &= \frac{p_{\hat{X}}(0)\, e^{-s}}{p_{\hat{X}}(0)\, e^{-s} + p_{\hat{X}}(1)}.
\end{align}
Since $e^{-s} = e^{-10} \approx 4.5 \times 10^{-5}$, the algorithm strongly
penalizes symbol errors, driving the test channel toward a near-identity matrix.

Figure~\ref{fig:ba_convergence} shows the convergence of the Blahut-Arimoto
algorithm for $p = 0.3$ and several values of the slope parameter $s$.
The algorithm converges rapidly, typically reaching machine precision within
$20$--$50$ iterations.
Larger values of $s$ correspond to lower target distortions and converge faster.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/ba_convergence.pdf}
    \caption{Convergence of the Blahut-Arimoto algorithm for $p = 0.3$ and slope
    parameters $s \in \{2, 5, 10, 20\}$.
    The rate converges monotonically to its final value within a few tens of iterations.}
    \label{fig:ba_convergence}
\end{figure}

To validate the algorithm, we sweep $s$ over a range of values and plot the
resulting $(D, R)$ pairs alongside the closed-form curve $\RD = \Hp - \HD$.
Figure~\ref{fig:ba_vs_closedform} confirms that the Blahut-Arimoto algorithm
recovers the exact rate-distortion function.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/ba_vs_closedform.pdf}
    \caption{Comparison of the Blahut-Arimoto computed rate-distortion points
    (circles) with the closed-form curve $\RD = \Hp - \HD$ (solid line)
    for $p = 0.3$.
    The agreement is exact to numerical precision.}
    \label{fig:ba_vs_closedform}
\end{figure}

% ---- 5.4 ----
\subsection{Historical Note}
\label{subsec:ba_history}

The algorithm was independently proposed by Blahut~\cite{blahut1972} and
Arimoto~\cite{arimoto1972} in 1972.
Blahut's formulation emphasized the Lagrangian dual structure, while Arimoto's
formulation was rooted in information geometry.
Csisz\'{a}r~\cite{csiszar1974} provided a rigorous convergence proof based on
alternating minimization of a divergence functional.
The Blahut-Arimoto algorithm remains the standard computational tool for
rate-distortion functions and channel capacities in information theory.


% ========================================================================
% SECTION 6: BEYOND THE ASYMPTOTIC LIMIT
% ========================================================================
\section{Beyond the Asymptotic Limit: Finite Block Length}
\label{sec:finite_blocklength}

In this section, we move beyond Shannon's asymptotic rate-distortion function
and develop the theory of finite block length lossy compression.
This is the mathematical core of the tutorial.

% ---- 6.1 ----
\subsection{The Gap Between Theory and Practice}
\label{subsec:gap}

The rate-distortion function $\RD$ tells us the ultimate limit of lossy
compression as the block length $n \to \infty$.
However, real systems operate with finite $n$.
A practical compression system might use blocks of $n = 100$ or $n = 1000$
symbols.
How much extra rate do we need compared to the Shannon limit?

Figure~\ref{fig:rate_vs_blocklength} illustrates the situation.
For a $\Ber(0.3)$ source with target distortion $D = 0.1$ and excess-distortion
probability $\varepsilon = 0.1$, the achievable rate at $n = 100$ is
significantly above $\RD$, but the gap narrows as $n$ grows.
Understanding the precise rate of this convergence is the goal of finite block
length theory.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/rate_vs_blocklength.pdf}
    \caption{The minimum achievable rate $R(n, D, \varepsilon)$ versus block length
    $n$ for a $\Ber(0.3)$ source with $D = 0.1$ and several values of the
    excess-distortion probability $\varepsilon$.
    The horizontal dashed line shows the Shannon limit $\RD$.
    The gap decays as $O(1/\sqrt{n})$.}
    \label{fig:rate_vs_blocklength}
\end{figure}

% ---- 6.2 ----
\subsection{The Finite Block Length Setup}
\label{subsec:fbl_setup}

We formalize the finite block length setting as follows.

\begin{definition}[$(n, M, D, \varepsilon)$ Code]
An $(n, M, D, \varepsilon)$ lossy source code consists of an encoder
$f_n : \calX^n \to \{1, \ldots, M\}$ and a decoder
$g_n : \{1, \ldots, M\} \to \calXhat^n$ such that the
\emph{excess-distortion probability} satisfies
\begin{equation}
\label{eq:excess_distortion}
\Prob\bigl(d(X^n, g_n(f_n(X^n))) > D\bigr) \leq \varepsilon.
\end{equation}
\end{definition}

Note the shift from the asymptotic setting.
Instead of requiring the \emph{average} distortion to be at most $D$, we
require that the distortion exceeds $D$ with probability at most $\varepsilon$.
This excess-distortion formulation is more natural for finite block lengths and
leads to cleaner second-order results.

The \emph{minimum achievable rate} at block length $n$ is
\begin{equation}
\label{eq:Rn}
R(n, D, \varepsilon) = \frac{1}{n} \log_2 M^*(n, D, \varepsilon),
\end{equation}
where $M^*(n, D, \varepsilon)$ is the smallest codebook size $M$ for which an
$(n, M, D, \varepsilon)$ code exists.
The fundamental result of finite block length theory is a precise
characterization of $R(n, D, \varepsilon)$.

% ---- 6.3 ----
\subsection{The $d$-Tilted Information}
\label{subsec:dtilted}

The central single-letter quantity in the finite block length analysis is the
\emph{$d$-tilted information}, introduced by Kostina and
Verd\'{u}~\cite{kostina2012}.

\begin{definition}[$d$-Tilted Information]
\label{def:dtilted}
For a source $X$ with distribution $p_X$, distortion measure $d$, and target
distortion $D$, the $d$-tilted information of a source realization $x$ is
\begin{equation}
\label{eq:dtilted}
\jmath_X(x, D) = \lambda^* D + \log_2 \frac{1}{\sum_{\hat{x} \in \calXhat}
p^*_{\hat{X}}(\hat{x})\, e^{-\lambda^* d(x, \hat{x})}},
\end{equation}
where $\lambda^* = -R'(D)$ is the negative slope of the rate-distortion
function (in nats, using the natural logarithm convention for the derivative)
and $p^*_{\hat{X}}$ is the reproduction distribution induced by the optimal
test channel.
\end{definition}

The $d$-tilted information has a compelling interpretation: it measures how
``difficult'' it is to compress a particular source realization $x$ to distortion
level $D$.
Different source symbols may be easier or harder to compress, and $\jmath_X(x, D)$
captures this variation.

A key property is that the expected $d$-tilted information equals the
rate-distortion function:
\begin{equation}
\label{eq:dtilted_mean}
\E[\jmath_X(X, D)] = \RD.
\end{equation}
This identity confirms that $\jmath_X(x, D)$ is the correct ``information density''
for the lossy compression problem.

For the $\Ber(p)$ source with Hamming distortion, the optimal reproduction
distribution is $p^*_{\hat{X}}(0) = (1-p)(1-D) + pD$ and
$p^*_{\hat{X}}(1) = p(1-D) + (1-p)D$, and the slope is
$\lambda^* = \ln\frac{1-D}{D}$.
The $d$-tilted information takes two values:
\begin{align}
\jmath_X(0, D) &= \log_2\frac{1-D}{D}\cdot D + \log_2 \frac{1}{p^*_{\hat{X}}(0) + p^*_{\hat{X}}(1)\frac{D}{1-D}}, \label{eq:jx0}\\
\jmath_X(1, D) &= \log_2\frac{1-D}{D}\cdot D + \log_2 \frac{1}{p^*_{\hat{X}}(0)\frac{D}{1-D} + p^*_{\hat{X}}(1)}. \label{eq:jx1}
\end{align}

Figure~\ref{fig:dtilted} shows the $d$-tilted information for both source
symbols as a function of $D$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/dtilted_info.pdf}
    \caption{The $d$-tilted information $\jmath_X(0, D)$ and $\jmath_X(1, D)$
    for a $\Ber(0.3)$ source with Hamming distortion.
    When $D$ is small, both values are close to $\Hp$ (the lossless rate).
    As $D$ increases toward $\min(p, 1-p) = 0.3$, both converge to zero.
    The gap between the two curves reflects the asymmetry of the source.}
    \label{fig:dtilted}
\end{figure}

% ---- 6.4 ----
\subsection{Dispersion: The Key Second-Order Quantity}
\label{subsec:dispersion}

The rate-distortion function $\RD$ is a first-order quantity: it captures the
leading-order behavior as $n \to \infty$.
The \emph{rate-distortion dispersion} is the second-order quantity that governs
how quickly the finite block length rate converges to $\RD$.

\begin{definition}[Rate-Distortion Dispersion]
\label{def:dispersion}
The rate-distortion dispersion at distortion level $D$ is
\begin{equation}
\label{eq:dispersion}
V(D) = \Var[\jmath_X(X, D)].
\end{equation}
\end{definition}

The dispersion measures how \emph{variable} the compression difficulty is across
source symbols.
If all source symbols are equally difficult to compress, then $V(D) = 0$ and
the convergence to $\RD$ is faster than $1/\sqrt{n}$.
If different symbols have very different compression difficulties, then $V(D)$
is large and the $1/\sqrt{n}$ penalty is more pronounced.

For the $\Ber(p)$ source, since $X$ takes only two values, we have
\begin{equation}
\label{eq:dispersion_bernoulli}
V(D) = p(1-p)\bigl(\jmath_X(1, D) - \jmath_X(0, D)\bigr)^2.
\end{equation}
This is simply the variance of a Bernoulli random variable that takes value
$\jmath_X(0, D)$ with probability $1-p$ and $\jmath_X(1, D)$ with probability $p$.

\begin{remark}
When $p = 1/2$, the source is symmetric: $\jmath_X(0, D) = \jmath_X(1, D)$
for all $D$, and therefore $V(D) = 0$.
Intuitively, every symbol of a fair coin is equally difficult to compress, so
there is no variability in compression difficulty.
This is a somewhat surprising consequence: for the fair Bernoulli source, the
convergence to $\RD$ is faster than $O(1/\sqrt{n})$.
\end{remark}

Figure~\ref{fig:dispersion} shows $V(D)$ as a function of $D$ for several
source biases.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/dispersion_curves.pdf}
    \caption{The rate-distortion dispersion $V(D)$ versus distortion $D$ for a
    $\Ber(p)$ source with $p \in \{0.1, 0.2, 0.3, 0.5\}$.
    For $p = 0.5$, the dispersion is identically zero (the source symbols are
    equally difficult to compress).
    For biased sources, the dispersion is largest at intermediate distortion levels.}
    \label{fig:dispersion}
\end{figure}

% ---- 6.5 ----
\subsection{The Normal Approximation}
\label{subsec:normal_approx}

We now state the central result of finite block length rate-distortion theory.
The minimum achievable rate at block length $n$ is characterized by the
following asymptotic expansion.

\begin{theorem}[Normal Approximation~\cite{kostina2012}]
\label{thm:normal_approx}
For a discrete memoryless source with rate-distortion function $\RD$ and
dispersion $V(D) > 0$, the minimum rate at block length $n$ and
excess-distortion probability $\varepsilon \in (0, 1)$ satisfies
\begin{equation}
\label{eq:normal_approx}
R(n, D, \varepsilon) = \RD + \sqrt{\frac{V(D)}{n}}\, \Qinv(\varepsilon)
+ O\!\left(\frac{\log n}{n}\right),
\end{equation}
where $\Qinv(\varepsilon)$ is the inverse of the Gaussian $Q$-function,
$Q(x) = \frac{1}{\sqrt{2\pi}} \int_x^{\infty} e^{-t^2/2}\, dt$.
\end{theorem}

The intuition behind this result comes from the Berry-Esseen central limit
theorem.
The total compression cost for an i.i.d.\ source sequence $X^n$ is
approximately $\sum_{i=1}^n \jmath_X(X_i, D)$, a sum of i.i.d.\ random
variables with mean $n\RD$ and variance $nV(D)$.
By the CLT, this sum is approximately Gaussian, and the excess-distortion
probability translates into a Gaussian tail probability.
The $\Qinv(\varepsilon)$ factor converts the target probability $\varepsilon$
into the number of standard deviations we must accommodate.

From an engineering standpoint, Theorem~\ref{thm:normal_approx} provides a
practical design rule: to operate within rate $\RD + \Delta R$ of the Shannon
limit with excess-distortion probability $\varepsilon$, we need a block length of
approximately
\begin{equation}
\label{eq:blocklength_design}
n \approx \frac{V(D) \bigl(\Qinv(\varepsilon)\bigr)^2}{(\Delta R)^2}.
\end{equation}
This expression reveals the fundamental trade-off among block length, rate
overhead, distortion, and reliability.

Figure~\ref{fig:finite_blocklength} shows the achievability bound, converse
bound, and normal approximation for a $\Ber(0.3)$ source.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/finite_blocklength.pdf}
    \caption{Finite block length bounds for a $\Ber(0.3)$ source with $D = 0.1$
    and $\varepsilon = 0.1$.
    The shaded region between the achievability bound (upper) and converse bound
    (lower) contains the true minimum rate $R(n, D, \varepsilon)$.
    The normal approximation (dashed) lies within this region.
    The horizontal line shows the Shannon limit $\RD$.}
    \label{fig:finite_blocklength}
\end{figure}

% ---- 6.6 ----
\subsection{The Tilting Distribution}
\label{subsec:tilting}

A deeper understanding of the finite block length result comes from the
\emph{tilting distribution}, which plays a central role in the converse proof.

\begin{definition}[Tilting Distribution]
The tilting distribution at distortion level $D$ is
\begin{equation}
\label{eq:tilting}
Q_D(x) = \frac{p_X(x)\, e^{\lambda^* \jmath_X(x, D)}}{\E\bigl[e^{\lambda^* \jmath_X(X, D)}\bigr]},
\end{equation}
where $\lambda^*$ is the optimal Lagrange multiplier.
\end{definition}

The tilting distribution is an exponential reweighting of the source
distribution.
It assigns higher probability to source symbols that are ``harder'' to compress
(those with larger $d$-tilted information).
In the converse proof, $Q_D$ represents the ``worst-case'' source distribution:
if we could not compress well under the original source, we certainly cannot
compress well under a distribution that emphasizes the difficult symbols.

For the $\Ber(p)$ source, the tilting distribution is another Bernoulli
distribution with a modified parameter.
This connection to exponential families is a recurring theme in information
theory and large deviations.

% ---- 6.7 ----
\subsection{Historical Note}
\label{subsec:fbl_history}

The study of finite block length performance in information theory dates back
to Strassen~\cite{strassen1962}, who established the $O(1/\sqrt{n})$ refinement
for hypothesis testing.
The channel coding counterpart was developed by Polyanskiy, Poor, and
Verd\'{u}~\cite{polyanskiy2010}, whose work on channel dispersion inspired
the lossy source coding treatment.
Kostina and Verd\'{u}~\cite{kostina2012} established the rate-distortion
dispersion and the normal approximation~(\ref{eq:normal_approx}) for general
discrete memoryless sources, building on contributions by
Ingber and Kochman~\cite{ingber2011}.
The $d$-tilted information framework provides a unified language for
second-order information theory.


% ========================================================================
% SECTION 7: NUMERICAL EXPLORATIONS
% ========================================================================
\section{Numerical Explorations}
\label{sec:numerical}

In this section, we present comprehensive numerical results that illustrate and
validate the theoretical developments of the preceding sections.
All figures are generated by the accompanying Python scripts in the
\texttt{scripts/} directory.

\subsection{Overview of Computational Tools}

We provide five Python scripts:
\begin{itemize}
    \item \texttt{rate\_distortion.py}: Computes and plots the binary entropy
          function and rate-distortion curves.
    \item \texttt{blahut\_arimoto.py}: Implements the Blahut-Arimoto algorithm
          with convergence tracking.
    \item \texttt{dispersion.py}: Computes the $d$-tilted information and
          rate-distortion dispersion.
    \item \texttt{finite\_blocklength.py}: Computes the normal approximation and
          finite block length bounds.
    \item \texttt{generate\_all\_figures.py}: Master script that generates all
          figures with consistent styling.
\end{itemize}

\subsection{Comprehensive Comparison}

Figure~\ref{fig:comprehensive} brings together the asymptotic rate-distortion
function and its finite block length refinements in a single plot.
For a $\Ber(0.3)$ source with excess-distortion probability $\varepsilon = 0.1$,
we show the achievable rate as a function of distortion $D$ for several block
lengths $n \in \{100, 500, 2000\}$.
As $n$ increases, the finite block length curves converge uniformly to the
Shannon limit $\RD$.
The gap is most pronounced at small distortions, where the dispersion $V(D)$ is
larger.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/comprehensive_comparison.pdf}
    \caption{The rate-distortion function $\RD$ (solid black) and the normal
    approximation $R(n, D, \varepsilon)$ for block lengths $n \in \{100, 500, 2000\}$,
    with $p = 0.3$ and $\varepsilon = 0.1$.
    The finite block length penalty is largest at small $D$ (high rate regime)
    and vanishes as $D \to \min(p, 1-p)$.}
    \label{fig:comprehensive}
\end{figure}

\subsection{Blahut-Arimoto Convergence}

The Blahut-Arimoto algorithm exhibits geometric convergence, as shown in
Figure~\ref{fig:ba_convergence}.
The convergence rate depends on the slope parameter $s$: larger $s$
(corresponding to smaller target distortions) leads to faster convergence in
terms of iteration count.
For all tested parameters, the algorithm reaches a relative error below $10^{-10}$
within $50$ iterations.


% ========================================================================
% SECTION 8: CONCLUSION
% ========================================================================
\section{Conclusion}
\label{sec:conclusion}

In this tutorial, we have presented a self-contained development of
rate-distortion theory for the $\Ber(p)$ source with Hamming distortion,
progressing from Shannon's asymptotic limit to the finite block length regime.

The key takeaways are threefold.
First, the rate-distortion function $\RD = \Hp - \HD$ provides a clean and
interpretable limit on lossy compression: the minimum rate is the source
entropy minus the noise entropy.
Second, the Blahut-Arimoto algorithm provides a reliable computational tool
for evaluating rate-distortion functions, even in settings where closed-form
solutions are unavailable.
Third, the finite block length theory, centered on the $d$-tilted information
$\jmath_X(x, D)$ and the dispersion $V(D)$, provides a precise characterization
of the penalty for operating at practical block lengths.
The normal approximation $R(n, D, \varepsilon) \approx \RD + \sqrt{V(D)/n}\, \Qinv(\varepsilon)$
is both elegant in form and useful in practice, offering a direct design rule
for system engineers.

Several directions remain for further investigation.
First, the extension to continuous sources, particularly the Gaussian source
with squared-error distortion, yields an analogous theory with its own
closed-form dispersion.
Second, the joint source-channel coding problem, where compression and
transmission are combined, introduces interactions between source and channel
dispersions that are not yet fully understood.
Third, the study of mismatched codes, where the encoder or decoder is designed
for the wrong source distribution, is of growing practical interest in the era
of learned compression.
It is our hope that this tutorial provides a foundation for readers to engage
with these and other frontiers of information theory.

% ========================================================================
% BIBLIOGRAPHY
% ========================================================================
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
